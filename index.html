<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Longchao Da </title> <meta name="author" content="Longchao Da"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/LongchaoHere/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/LongchaoHere/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/LongchaoHere/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/LongchaoHere/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://longchaoda.github.io/LongchaoHere/"> <script src="/LongchaoHere/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/LongchaoHere/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/LongchaoHere/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/LongchaoHere/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/LongchaoHere/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/LongchaoHere/service/">Service </a> </li> <li class="nav-item "> <a class="nav-link" href="/LongchaoHere/teaching/">Teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Longchao</span> Da </h1> <p class="desc"><a href="https://www.asu.edu/" rel="external nofollow noopener" target="_blank">Affiliations</a>. Arizona State University</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/LongchaoHere/assets/img/prof_pic-480.webp 480w,/LongchaoHere/assets/img/prof_pic-800.webp 800w,/LongchaoHere/assets/img/prof_pic-1400.webp 1400w," sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/LongchaoHere/assets/img/prof_pic.png?5a164cbaf94c9302d4f581b25427022e" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_pic.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <p>BYENG 4th floor,</p> <p>699 Mill Ave.</p> <p>Tempe, Arizona.</p> </div> </div> <div class="clearfix"> <p>Hey there👋, I’m a Ph.D. student at the Computer Science Department, Arizona State University under the supervision of <a href="https://www.public.asu.edu/~hwei27/index.html" rel="external nofollow noopener" target="_blank">Professor. Hua Wei</a>. Generally, I’m dedicated to the study of LLMs, Reinforcement Learning, Data Mining, and Trustworthy Policy Evaluation &amp; Deployment.</p> <p>“AI should serve for the real world problems. — Andrew Ng”</p> <p>My research roadmap is as shown:</p> <p><img src="assets/img/roadmap.jpg" alt="Roadmap" style="display: block; margin: 0 auto; width: 50%;"></p> <p><br> I have been working on Foundation Models &amp; LLM Trustworthiness from two aspects: solving bottleneck (BN) problems and generalize to real applications (Apply). <br></p> <p><strong>1) LLM reasoning uncertainty quantification by linguistics entailment information(BN)</strong>, <br> <strong>2) Trustworthy LLM with RAG enhanced evidential answering(BN)</strong>, <br> <strong>3) Free-form text prompt segmentation(BN).</strong> <br> <strong>4) Augmented LLM agent for domain specific tasks with tool embodiment(Apply)</strong>. <br></p> <p><br> Towards more deployable policy learning (i.e., RL). My previous work is trying to achieve it from: <br></p> <p><strong>1) Designing more realistic simulations</strong>, <br> <strong>2) Grounding the sim-to-real transition gaps</strong>, <br> <strong>3) Conducting more comprehensive policy evaluations prior to deployment</strong>. <br></p> <div style="max-width: 65%; height: 300px; padding: 15px; border: 1px solid #ccc; margin: 20px auto; background-color: #f9f9f9; border-radius: 8px; box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.1); float: left; overflow-y: auto"> <p><strong>Recent News:</strong></p> <div style="display: grid; grid-template-columns: 70px auto; grid-gap: 5px;"> <div>[2025-03]</div> <div>A poster on LibSignal++ Sim-to-Real physical testbed is accepted by ICCPS 2025 🎉.</div> <div>[2025-02]</div> <div>Received the SDM Travel Award ✈️ from <a href="https://www.siam.org/conferences-events/conference-support/travel-and-registration-support/" rel="external nofollow noopener" target="_blank"> SIAM </a>, and Experiential Learning Grant from <a href="https://students.engineering.asu.edu/scholarships-funding/experiential-learning-grant/" rel="external nofollow noopener" target="_blank"> ASU </a>, see you in Alexandria Virginia, U.S.</div> <div>[2025-01]</div> <div>Starting my internship 💼 at Honda Research Institute, San Jose, California.</div> <div>[2025-01]</div> <div>Two papers 📰 are accepted to SDM 2025 🎉🎉.</div> <div>[2025-01]</div> <div>Glad to receive the ASU Ph.D. Fellowship Award 🏆.</div> <div>[2024-09]</div> <div>I have made a project page for our paper: <a href="https://arxiv.org/pdf/2410.12831" rel="external nofollow noopener" target="_blank"> Segment as You Wish--Free-Form Language-Based Segmentation for Medical Images </a>, please visit the link at: <a href="https://longchaoda.github.io/segmentAsYouWish.github.io/"> check out here! </a> </div> <div>[2024-10]</div> <div>I am invited to join Program Committee at SDM 2025.</div> <div>[2024-10]</div> <div>I will serve as Session Chair at CIKM24. <br>*Room: 110A. <br>*Time: Tuesday, 11:00 - 12:45. <br>Topic FP2: Urban Systems.</div> <div>[2024-09]</div> <div>One paper 📰 is accepted to NeurIPS 2024.</div> <div>[2024-09]</div> <div>Finished my Internship at GE Healthcare <a href="https://github.com/LongchaoDa/papers/blob/main/Segment%20as%20You%20Wish%20-%20Free-Form%20Language-Based%20Segmentation%20for%20Medical%20Images.pdf" rel="external nofollow noopener" target="_blank"> paper </a>, thanks <a href="https://sites.google.com/view/danicaxiao/home" rel="external nofollow noopener" target="_blank"> Danica Xiao</a>, <a href="https://rui1521.github.io/online-cv/" rel="external nofollow noopener" target="_blank"> Rui Wang </a> and <a href="https://xuxiaojian.github.io/" rel="external nofollow noopener" target="_blank"> Xiaojian Xu </a>, I had a great time with you! </div> <div>[2024-08]</div> <div>I am selected as NSF travel awardee to attend CIKM2024 ✈️. See you in Boise, Idaho, USA.</div> <div>[2024-08]</div> <div>One paper 📰 is accepted to ITSC 2024.</div> <div>[2024-08]</div> <div>One paper 📰 is accepted to CIKM 2024.</div> <div>[2024-08]</div> <div>Invited to give a talk at <a href="https://sites.google.com/view/workshop-itsc-2024/%E9%A6%96%E9%A1%B5" rel="external nofollow noopener" target="_blank"> ITSC2024</a> about advanced simulation and policy learning. <a href="https://youtu.be/Le3BjhsYe2Q" rel="external nofollow noopener" target="_blank"> Video</a> </div> <div>[2024-05]</div> <div>One paper 📰 is accepted to ECML-PKDD 2024.</div> <div>[2024-05]</div> <div>I was selected (30 Ph.D. students nation-wide) to attend <a href="https://ai-score.github.io/" rel="external nofollow noopener" target="_blank">AI-SCORE</a> at University of Maryland held by <a href="https://www.rhsmith.umd.edu/directory/michael-fu" rel="external nofollow noopener" target="_blank">Michael Fu - UMD</a>, <a href="https://idm-lab.org/" rel="external nofollow noopener" target="_blank">Sven Koenig - USC</a> and <a href="https://www.engineering.cornell.edu/faculty-directory/david-b-shmoys" rel="external nofollow noopener" target="_blank">David Shmoys - Cornell</a> </div> <div>[2024-01]</div> <div>Two first author papers 📰 are accepted to AAAI 2024.</div> <div>[2023-11]</div> <div>Our paper on <a href="https://darl-libsignal.github.io/index.html" rel="external nofollow noopener" target="_blank">"LibSignal"</a> is accepted to Machine Learning - Springer.</div> <div>[2023-11]</div> <div>Successfully hosted my first Tutorial Session 📖 <a href="https://darl-libsignal.github.io/itsc2023.html" rel="external nofollow noopener" target="_blank">"Cross-simulator Datasets and Evaluations for Traffic Control Policies"</a> in <a href="https://2023.ieee-itsc.org/accepted-tutorials/" rel="external nofollow noopener" target="_blank">IEEE-ITSC</a> 2023, Bilbao, Spain.</div> </div> </div> <p><br> <br></p> <div id="clustrmaps-container" style="width: 275px; height: 300px; overflow: hidden; float: right;"> <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&amp;w=a&amp;t=n&amp;d=f7XCCDBy6e2xZcUt7nrq9L-5IhotWsRN7V4Tk1tpy7c&amp;co=bfdbef"></script> <p style="font-family: 'Courier New', Courier, monospace; font-size: 16px; margin-top: 10px; font-weight: bold;"> Visitors Distribution </p> </div> </div> <h2> <a href="/LongchaoHere/publications/" style="color: inherit">Selected Publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">pre-print</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/LongchaoHere/assets/img/publication_preview/reasonUQ-480.webp 480w,/LongchaoHere/assets/img/publication_preview/reasonUQ-800.webp 800w,/LongchaoHere/assets/img/publication_preview/reasonUQ-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/LongchaoHere/assets/img/publication_preview/reasonUQ.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="reasonUQ.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="da2025understanding" class="col-sm-8"> <div class="title">Understanding the Uncertainty of LLM Explanations: A Perspective Based on Reasoning Topology</div> <div class="author"> <em>Longchao Da</em>, Xiaoou Liu, Jiaxin Dai , and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Lu Cheng, Yaqing Wang, Hua Wei' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em></em> 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2502.17026" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks due to large training datasets and powerful transformer architecture. However, the reliability of responses from LLMs remains a question. Uncertainty quantification (UQ) of LLMs is crucial for ensuring their reliability, especially in areas such as healthcare, finance, and decision-making. Existing UQ methods primarily focus on semantic similarity, overlooking the deeper knowledge dimensions embedded in responses. We introduce a multi-dimensional UQ framework that integrates semantic and knowledge-aware similarity analysis. By generating multiple responses and leveraging auxiliary LLMs to extract implicit knowledge, we construct separate similarity matrices and apply tensor decomposition to derive a comprehensive uncertainty representation. This approach disentangles overlapping information from both semantic and knowledge dimensions, capturing both semantic variations and factual consistency, leading to more accurate UQ. Our empirical evaluations demonstrate that our method outperforms existing techniques in identifying uncertain responses, offering a more robust framework for enhancing LLM reliability in high-stakes applications.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">da2025understanding</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Understanding the Uncertainty of LLM Explanations: A Perspective Based on Reasoning Topology}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Da, Longchao and Liu, Xiaoou and Dai, Jiaxin and Cheng, Lu and Wang, Yaqing and Wei, Hua}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">pre-print</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/LongchaoHere/assets/img/publication_preview/thumbSim2real-480.webp 480w,/LongchaoHere/assets/img/publication_preview/thumbSim2real-800.webp 800w,/LongchaoHere/assets/img/publication_preview/thumbSim2real-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/LongchaoHere/assets/img/publication_preview/thumbSim2real.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="thumbSim2real.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="da2025survey" class="col-sm-8"> <div class="title">A Survey of Sim-to-Real Methods in RL: Progress, Prospects and Challenges with Foundation Models</div> <div class="author"> <em>Longchao Da</em>, Justin Turnau, Thirulogasankar Pranav Kutralingam , and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Alvaro Velasquez, Paulo Shakarian, Hua Wei' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em></em> 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2502.13187" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Deep Reinforcement Learning (RL) has been explored and verified to be effective in solving decision-making tasks in various domains, such as robotics, transportation, recommender systems, etc. It learns from the interaction with environments and updates the policy using the collected experience. However, due to the limited real-world data and unbearable consequences of taking detrimental actions, the learning of RL policy is mainly restricted within the simulators. This practice guarantees safety in learning but introduces an inevitable sim-to-real gap in terms of deployment, thus causing degraded performance and risks in execution. There are attempts to solve the sim-to-real problems from different domains with various techniques, especially in the era with emerging techniques such as large foundations or language models that have cast light on the sim-to-real. This survey paper, to the best of our knowledge, is the first taxonomy that formally frames the sim-to-real techniques from key elements of the Markov Decision Process (State, Action, Transition, and Reward). Based on the framework, we cover comprehensive literature from the classic to the most advanced methods including the sim-to-real techniques empowered by foundation models, and we also discuss the specialties that are worth attention in different domains of sim-to-real problems. Then we summarize the formal evaluation process of sim-to-real performance with accessible code or benchmarks. The challenges and opportunities are also presented to encourage future exploration of this direction. We are actively maintaining a to include the most up-to-date sim-to-real research outcomes to help the researchers in their work.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">da2025survey</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Survey of Sim-to-Real Methods in RL: Progress, Prospects and Challenges with Foundation Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Da, Longchao and Turnau, Justin and Kutralingam, Thirulogasankar Pranav and Velasquez, Alvaro and Shakarian, Paulo and Wei, Hua}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">pre-print</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/LongchaoHere/assets/img/publication_preview/uncerLLM-480.webp 480w,/LongchaoHere/assets/img/publication_preview/uncerLLM-800.webp 800w,/LongchaoHere/assets/img/publication_preview/uncerLLM-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/LongchaoHere/assets/img/publication_preview/uncerLLM.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="uncerLLM.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="da2024llm" class="col-sm-8"> <div class="title">LLM Uncertainty Quantification through Directional Entailment Graph and Claim Level Response Augmentation</div> <div class="author"> <em>Longchao Da</em>, Tiejin Chen, Lu Cheng , and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Hua Wei' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em></em> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2407.00994" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>The Large language models (LLMs) have showcased superior capabilities in sophisticated tasks across various domains, stemming from basic question-answer (QA), they are nowadays used as decision assistants or explainers for unfamiliar content. However, they are not always correct due to the data sparsity in specific domain corpus, or the model’s hallucination problems. Given this, how much should we trust the responses from LLMs? This paper studies the question in two aspects, first, we find measuring directly on raw responses from LLMs is easy to be vague due to the lack of context, second this paper identifies the oversight in existing work regarding the neglect of directional relationships between sentences. To tackle the challenges, we first provide a claim-based response augmentation, by completing the fine-grained claims in raw response, and augmenting into a more explicit description, it removes the randomness, improves the completeness, and provides a response set that robustly represents the knowledge of LLMs. Then, second, methodologically we present a way to evaluate the uncertainty that captures the directional instability between sentences by constructing a directional graph using entailment probabilities, and conducting Random Walk Laplacian given the asymmetric property in the directed graph, then the uncertainty is aggregated by the derived eigenvalues from the Laplacian process. We conducted extensive experiments and empirically demonstrated the superiority of our proposed solutions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">da2024llm</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{LLM Uncertainty Quantification through Directional Entailment Graph and Claim Level Response Augmentation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Da, Longchao and Chen, Tiejin and Cheng, Lu and Wei, Hua}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">pre-print</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/LongchaoHere/assets/img/publication_preview/segmed-480.webp 480w,/LongchaoHere/assets/img/publication_preview/segmed-800.webp 800w,/LongchaoHere/assets/img/publication_preview/segmed-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/LongchaoHere/assets/img/publication_preview/segmed.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="segmed.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="da2024prompt" class="col-sm-8"> <div class="title">Segment as You Wish–Free-Form Language-Based Segmentation for Medical Images</div> <div class="author"> <em>Longchao Da</em>, Rui Wang, Xiaojian Xu , and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Parminder Bhatia, Taha Kass-Hout, Hua Wei, Cao Xiao' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>In </em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://longchaoda.github.io/segmentAsYouWish.github.io/" class="btn btn-sm z-depth-0" role="button">HTML</a> <a href="https://youtu.be/CvAVgxK9eZI?si=h8ktpJjkAAK3mkbt" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>Medical imaging is crucial for diagnosing a patient’s health condition, and accurate segmentation of these images is essential for isolating regions of interest to ensure precise diagnosis and treatment planning. Existing methods primarily rely on bounding boxes or point-based prompts, while few have explored text-related prompts, despite clinicians often describing their observations and instructions in natural language. To address this gap, we first propose a RAG-based free-form text prompt generator, that leverages the domain corpus to generate diverse and realistic descriptions. Then, we introduce FLanS, a novel medical image segmentation model that handles various free-form text prompts, including professional anatomy-informed queries, anatomy-agnostic position-driven queries, and anatomy-agnostic size-driven queries. Additionally, our model also incorporates a symmetry-aware canonicalization module to ensure consistent, accurate segmentations across varying scan orientations and reduce confusion between the anatomical position of an organ and its appearance in the scan. FLanS is trained on a large-scale dataset of over 100k medical images from 7 public datasets. Comprehensive experiments demonstrate the model’s superior language understanding and segmentation precision, along with a deep comprehension of the relationship between them, outperforming SOTA baselines on both in-domain and out-of-domain datasets.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">da2024prompt</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Segment as You Wish--Free-Form Language-Based Segmentation for Medical Images}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Da, Longchao and Wang, Rui and Xu, Xiaojian and Bhatia, Parminder and Kass-Hout, Taha and Wei, Hua and Xiao, Cao}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">pre-print</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/LongchaoHere/assets/img/publication_preview/deepUncer-480.webp 480w,/LongchaoHere/assets/img/publication_preview/deepUncer-800.webp 800w,/LongchaoHere/assets/img/publication_preview/deepUncer-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/LongchaoHere/assets/img/publication_preview/deepUncer.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="deepUncer.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="young2024flexible" class="col-sm-8"> <div class="title">Flexible Heteroscedastic Count Regression with Deep Double Poisson Networks</div> <div class="author"> Spencer Young, Porter Jenkins, <em>Longchao Da</em> , and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Jeff Dotson, Hua Wei' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em></em> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2406.09262" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Neural networks that can produce accurate, input-conditional uncertainty representations are critical for real-world applications. Recent progress on heteroscedastic continuous regression has shown great promise for calibrated uncertainty quantification on complex tasks, like image regression. However, when these methods are applied to discrete regression tasks, such as crowd counting, ratings prediction, or inventory estimation, they tend to produce predictive distributions with numerous pathologies. We propose to address these issues by training a neural network to output the parameters of a Double Poisson distribution, which we call the Deep Double Poisson Network (DDPN). In contrast to existing methods that are trained to minimize Gaussian negative log likelihood (NLL), DDPNs produce a proper probability mass function over discrete output. Additionally, DDPNs naturally model under-, over-, and equi-dispersion, unlike networks trained with the more rigid Poisson and Negative Binomial parameterizations. We show DDPNs 1) vastly outperform existing discrete models; 2) meet or exceed the accuracy and flexibility of networks trained with Gaussian NLL; 3) produce proper predictive distributions over discrete counts; and 4) exhibit superior out-of-distribution detection. DDPNs can easily be applied to a variety of count regression datasets including tabular, image, point cloud, and text data.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">young2024flexible</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Flexible Heteroscedastic Count Regression with Deep Double Poisson Networks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Young, Spencer and Jenkins, Porter and Da, Longchao and Dotson, Jeff and Wei, Hua}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CIKM24</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/LongchaoHere/assets/img/publication_preview/evidenceChat-480.webp 480w,/LongchaoHere/assets/img/publication_preview/evidenceChat-800.webp 800w,/LongchaoHere/assets/img/publication_preview/evidenceChat-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/LongchaoHere/assets/img/publication_preview/evidenceChat.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="evidenceChat.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="da2024evidencechat" class="col-sm-8"> <div class="title">EvidenceChat: A RAG Enhanced LLM Framework for Trustworthy and Evidential Response Generation</div> <div class="author"> <em>Longchao Da</em>, Parth Mitesh Shah, Ananya Singh , and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Hua Wei' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em></em> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.researchgate.net/profile/Parth-Shah-142/publication/385173895_EvidenceChat_A_RAG_Enhanced_LLM_Framework_for_Trustworthy_and_Evidential_Response_Generation/links/671983dadf4b534d4eeddf46/EvidenceChat-A-RAG-Enhanced-LLM-Framework-for-Trustworthy-and-Evidential-Response-Generation.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>The Large Language Model has become an important assistant for many of human’s decision-making. However, a side note almost comes along with every single LLM:‘LLMs can make mistakes. Be careful with important info’. This reveals the fact that not all of the information from LLMs is trustworthy, and people still need to judge by themselves. Yet, the hallucination is so powerful that a made-up conclusion could even come with a seemingly plausible reason, which brings intricate challenges and a trust crisis among users. This paper proposes EvidenceChat, a framework that tackles this issue in a retrieval-augmented fashion, specifically, when the user uploads a material document, an indexed vector space will be constructed based on text embeddings, which helps construct a retrieval-augmented agent, enhancing the agent’s responses with additional knowledge beyond its training corpus. Then, we propose a three-step evidential retrieval method to accurately locate the evidence of a language agent’s conclusion in the source context: Chain-of-Thought (CoT) logic generation, top-k relevant chunk vector matching, and a self-reflection-based precise sentence identification. We demonstrate that our method improves the existing models’ performance in terms of identifying the exact evidence in a free-form context, providing a reliable way to examine the resources of LLM’s conclusion and help with the judgment of the trustworthiness. The dataset and code will be released.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">da2024evidencechat</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{EvidenceChat: A RAG Enhanced LLM Framework for Trustworthy and Evidential Response Generation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Da, Longchao and Shah, Parth Mitesh and Singh, Ananya and Wei, Hua}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">SDM25</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/LongchaoHere/assets/img/publication_preview/multiagentCars-480.webp 480w,/LongchaoHere/assets/img/publication_preview/multiagentCars-800.webp 800w,/LongchaoHere/assets/img/publication_preview/multiagentCars-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/LongchaoHere/assets/img/publication_preview/multiagentCars.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="multiagentCars.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="yao2024comal" class="col-sm-8"> <div class="title">CoMAL: Collaborative Multi-Agent Large Language Models for Mixed-Autonomy Traffic</div> <div class="author"> <em>Longchao Da</em>, Huaiyuan* Yao, Vishnu Nandam , and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Justin Turnau, Zhiwei Liu, Linsey Pang, Hua Wei' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em></em> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2410.14368" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>The integration of autonomous vehicles into urban traffic has great potential to improve efficiency by reducing congestion and optimizing traffic flow systematically. In this paper, we introduce CoMAL (Collaborative Multi-Agent LLMs), a framework designed to address the mixed-autonomy traffic problem by collaboration among autonomous vehicles to optimize traffic flow. CoMAL is built upon large language models, operating in an interactive traffic simulation environment. It utilizes a Perception Module to observe surrounding agents and a Memory Module to store strategies for each agent. The overall workflow includes a Collaboration Module that encourages autonomous vehicles to discuss the effective strategy and allocate roles, a reasoning engine to determine optimal behaviors based on assigned roles, and an Execution Module that controls vehicle actions using a hybrid approach combining rule-based models. Experimental results demonstrate that CoMAL achieves superior performance on the Flow benchmark. Additionally, we evaluate the impact of different language models and compare our framework with reinforcement learning approaches. It highlights the strong cooperative capability of LLM agents and presents a promising solution to the mixed-autonomy traffic challenge</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">yao2024comal</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{CoMAL: Collaborative Multi-Agent Large Language Models for Mixed-Autonomy Traffic}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Da, Longchao and Yao, Huaiyuan and Nandam, Vishnu and Turnau, Justin and Liu, Zhiwei and Pang, Linsey and Wei, Hua}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">SDM25</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/LongchaoHere/assets/img/publication_preview/privacy-480.webp 480w,/LongchaoHere/assets/img/publication_preview/privacy-800.webp 800w,/LongchaoHere/assets/img/publication_preview/privacy-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/LongchaoHere/assets/img/publication_preview/privacy.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="privacy.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="chen2024privacy" class="col-sm-8"> <div class="title">Privacy-preserving Fine-tuning of Large Language Models through Flatness</div> <div class="author"> Tiejin Chen, <em>Longchao Da</em>, Huixue Zhou , and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Pingzhi Li, Kaixiong Zhou, Tianlong Chen, Hua Wei' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em></em> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2403.04124" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>The privacy concerns associated with the use of Large Language Models (LLMs) have grown recently with the development of LLMs such as ChatGPT. Differential Privacy (DP) techniques are explored in existing work to mitigate their privacy risks at the cost of generalization degradation. Our paper reveals that the flatness of DP-trained models’ loss landscape plays an essential role in the trade-off between their privacy and generalization. We further propose a holistic framework to enforce appropriate weight flatness, which substantially improves model generalization with competitive privacy preservation. It innovates from three coarse-to-grained levels, including perturbation-aware min-max optimization on model weights within a layer, flatness-guided sparse prefix-tuning on weights across layers, and weight knowledge distillation between DP &amp; non-DP weights copies. Comprehensive experiments of both black-box and white-box scenarios are conducted to demonstrate the effectiveness of our proposal in enhancing generalization and maintaining DP characteristics. For instance, on text classification dataset QNLI, DP-Flat achieves similar performance with non-private full fine-tuning but with DP guarantee under privacy budget, and even better performance given higher privacy budgets. Codes are provided in the supplement.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">chen2024privacy</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Privacy-preserving Fine-tuning of Large Language Models through Flatness}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chen, Tiejin and Da, Longchao and Zhou, Huixue and Li, Pingzhi and Zhou, Kaixiong and Chen, Tianlong and Wei, Hua}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS24</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/LongchaoHere/assets/img/publication_preview/regExplain-480.webp 480w,/LongchaoHere/assets/img/publication_preview/regExplain-800.webp 800w,/LongchaoHere/assets/img/publication_preview/regExplain-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/LongchaoHere/assets/img/publication_preview/regExplain.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="regExplain.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="da2024prompu" class="col-sm-8"> <div class="title">RegExplainer: Generating Explanations for Graph Neural Networks in Regression Tasks</div> <div class="author"> Jiaxing Zhang, Zhuomin Chen, Hao Mei , and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Longchao Da, Dongsheng Luo, Hua Wei' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 2024 Neural Information Processing Systems</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://neurips.cc/virtual/2024/poster/94251" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Graph regression is a fundamental task and has received increasing attention in a wide range of graph learning tasks. However, the inference process is often not interpretable. Most existing explanation techniques are limited to understanding GNN behaviors in classification tasks. In this work, we seek an explanation to interpret the graph regression models (XAIG-R). We show that existing methods overlook the distribution shifting and continuously ordered decision boundary, which hinders them away from being applied in the regression tasks. To address these challenges, we propose a novel objective based on the information bottleneck theory and introduce a new mix-up framework, which could support various GNNs in a model-agnostic manner. We further present a contrastive learning strategy to tackle the continuously ordered labels in regression task. To empirically verify the effectiveness of the proposed method, we introduce three benchmark datasets and a real-life dataset for evaluation. Extensive experiments show the effectiveness of the proposed method in interpreting GNN models in regression tasks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">da2024prompu</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{RegExplainer: Generating Explanations for Graph Neural Networks in Regression Tasks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Jiaxing and Chen, Zhuomin and Mei, Hao and Da, Longchao and Luo, Dongsheng and Wei, Hua}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2024 Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">AAAI24</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/LongchaoHere/assets/img/publication_preview/PromptGAT-480.webp 480w,/LongchaoHere/assets/img/publication_preview/PromptGAT-800.webp 800w,/LongchaoHere/assets/img/publication_preview/PromptGAT-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/LongchaoHere/assets/img/publication_preview/PromptGAT.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="PromptGAT.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="da2024prompv" class="col-sm-8"> <div class="title">Prompt to Transfer: Sim-to-Real Transfer for Traffic Signal Control with Prompt Learning</div> <div class="author"> <em>Longchao Da</em>, Minquan Gao, Hao Mei , and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Hua Wei' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Proceedings of the AAAI Conference on Artificial Intelligence</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/27758" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Numerous solutions are proposed for the Traffic Signal Con- trol (TSC) tasks aiming to provide efficient transportation and alleviate traffic congestion. Recently, promising results have been attained by Reinforcement Learning (RL) methods through trial and error in simulators, bringing confidence in solving cities’ congestion problems. However, performance gaps still exist when simulator-trained policies are deployed to the real world. This issue is mainly introduced by the sys- tem dynamic difference between the training simulators and the real-world environments. In this work, we leverage the knowledge of Large Language Models (LLMs) to understand and profile the system dynamics by a prompt-based grounded action transformation to bridge the performance gap. Specifi- cally, this paper exploits the pre-trained LLM’s inference abil- ity to understand how traffic dynamics change with weather conditions, traffic states, and road types. Being aware of the changes, the policies’ action is taken and grounded based on realistic dynamics, thus helping the agent learn a more realistic policy. We conduct experiments on four different scenarios to show the effectiveness of the proposed PromptGAT’s ability to mitigate the performance gap of reinforcement learning from simulation to reality (sim-to-real).</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">da2024prompv</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Prompt to Transfer: Sim-to-Real Transfer for Traffic Signal Control with Prompt Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Da, Longchao and Gao, Minquan and Mei, Hao and Wei, Hua}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the AAAI Conference on Artificial Intelligence}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1609/aaai.v38i1.27758}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">AAAI24</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/LongchaoHere/assets/img/publication_preview/POPR-480.webp 480w,/LongchaoHere/assets/img/publication_preview/POPR-800.webp 800w,/LongchaoHere/assets/img/publication_preview/POPR-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/LongchaoHere/assets/img/publication_preview/POPR.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="POPR.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="da2024probabilistic" class="col-sm-8"> <div class="title">Probabilistic Offline Policy Ranking with Approximate Bayesian Computation</div> <div class="author"> <em>Longchao Da</em>, Porter Jenkins, Trevor Schwantes , and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Jeffrey Dotson, Hua Wei' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the AAAI Conference on Artificial Intelligence</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/30019" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>In practice, it is essential to compare and rank candidate policies offline before real-world deployment for safety and reliability. Prior work seeks to solve this offline policy ranking (OPR) problem through value-based methods, such as Off-policy evaluation (OPE). However, they fail to analyze special case performance (e.g., worst or best cases), due to the lack of holistic characterization of policies’ performance. It is even more difficult to estimate precise policy values when the reward is not fully accessible under sparse settings. In this paper, we present Probabilistic Offline Policy Ranking (POPR), a framework to address OPR problems by leveraging expert data to characterize the probability of a candidate policy behaving like experts, and approximating its entire performance posterior distribution to help with ranking. POPR does not rely on value estimation, and the derived performance posterior can be used to distinguish candidates in worst-, best-, and average-cases. To estimate the posterior, we propose POPR-EABC, an Energy-based Approximate Bayesian Computation (ABC) method conducting likelihood-free inference. POPR-EABC reduces the heuristic nature of ABC by a smooth energy function, and improves the sampling efficiency by a pseudo-likelihood. We empirically demonstrate that POPR-EABC is adequate for evaluating policies in both discrete and continuous action spaces across various experiment environments, and facilitates probabilistic comparisons of candidate policies before deployment.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">da2024probabilistic</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Probabilistic Offline Policy Ranking with Approximate Bayesian Computation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Da, Longchao and Jenkins, Porter and Schwantes, Trevor and Dotson, Jeffrey and Wei, Hua}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the AAAI Conference on Artificial Intelligence}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{38}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{18}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{20370--20378}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1609/aaai.v38i18.30019}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IJMLC</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/LongchaoHere/assets/img/publication_preview/OpenTI-480.webp 480w,/LongchaoHere/assets/img/publication_preview/OpenTI-800.webp 800w,/LongchaoHere/assets/img/publication_preview/OpenTI-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/LongchaoHere/assets/img/publication_preview/OpenTI.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="OpenTI.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="da2024open" class="col-sm-8"> <div class="title">Open-ti: Open traffic intelligence with augmented language model</div> <div class="author"> <em>Longchao Da</em>, Kuanru Liou, Tiejin Chen , and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Xuesong Zhou, Xiangyong Luo, Yezhou Yang, Hua Wei' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>International Journal of Machine Learning and Cybernetics</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/article/10.1007/s13042-024-02190-8" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://youtu.be/fWVt1p6EIjY?feature=shared" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>Transportation has greatly benefited the cities’ development in the modern civilization process. Intelligent transportation, leveraging advanced computer algorithms, could further increase people’s daily commuting efficiency. However, intelligent transportation, as a cross-discipline, often requires practitioners to comprehend complicated algorithms and obscure neural networks, bringing a challenge for the advanced techniques to be trusted and deployed in practical industries. Recognizing the expressiveness of the pre-trained large language models, especially the potential of being augmented with abilities to understand and execute intricate commands, we introduce Open-TI. Serving as a bridge to mitigate the industry-academic gap, Open-TI is an innovative model targeting the goal of Turing Indistinguishable Traffic Intelligence, it is augmented with the capability to harness external traffic analysis packages based on existing conversations. Marking its distinction, Open-TI is the first method capable of conducting exhaustive traffic analysis from scratch—spanning from map data acquisition to the eventual execution in complex simulations. Besides, Open-TI is able to conduct task-specific embodiment like training and adapting the traffic signal control policies (TSC), explore demand optimizations, etc. Furthermore, we explored the viability of LLMs directly serving as control agents, by understanding the expected intentions from Open-TI, we designed an agent-to-agent communication mode to support Open-TI conveying messages to ChatZero (control agent), and then the control agent would choose from the action space to proceed the execution. We eventually provide the formal implementation structure, and the open-ended design invites further community-driven enhancements. A demo video is provided at: https://youtu.be/pZ4-5PXz9Xs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">da2024open</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Open-ti: Open traffic intelligence with augmented language model}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Da, Longchao and Liou, Kuanru and Chen, Tiejin and Zhou, Xuesong and Luo, Xiangyong and Yang, Yezhou and Wei, Hua}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Journal of Machine Learning and Cybernetics}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--26}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/s13042-024-02190-8}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CDC23</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/LongchaoHere/assets/img/publication_preview/CDC23-480.webp 480w,/LongchaoHere/assets/img/publication_preview/CDC23-800.webp 800w,/LongchaoHere/assets/img/publication_preview/CDC23-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/LongchaoHere/assets/img/publication_preview/CDC23.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="CDC23.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="da2023uncertainty" class="col-sm-8"> <div class="title">Uncertainty-aware Grounded Action Transformation towards Sim-to-Real Transfer for Traffic Signal Control</div> <div class="author"> <em>Longchao Da</em>, Hao Mei, Romir Sharma , and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Hua Wei' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In 2023 62nd IEEE Conference on Decision and Control (CDC)</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/10383645" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Traffic signal control (TSC) is a complex and important task that affects the daily lives of millions of people. Reinforcement Learning (RL) has shown promising results in optimizing traffic signal control, but current RL-based TSC methods are mainly trained in simulation and suffer from the performance gap between simulation and the real world. In this paper, we propose a simulation-to-real-world (sim-to-real) transfer approach called UGAT, which transfers a learned policy trained from a simulated environment to a real-world environment by dynamically transforming actions in the simulation with uncertainty to mitigate the domain gap of transition dynamics. We evaluate our method on a simulated traffic environment and show that it significantly improves the performance of the transferred RL policy in the real world.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">da2023uncertainty</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Uncertainty-aware Grounded Action Transformation towards Sim-to-Real Transfer for Traffic Signal Control}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Da, Longchao and Mei, Hao and Sharma, Romir and Wei, Hua}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2023 62nd IEEE Conference on Decision and Control (CDC)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1124--1129}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ECML-PKDD24</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/LongchaoHere/assets/img/publication_preview/ECMLPKDD24-480.webp 480w,/LongchaoHere/assets/img/publication_preview/ECMLPKDD24-800.webp 800w,/LongchaoHere/assets/img/publication_preview/ECMLPKDD24-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/LongchaoHere/assets/img/publication_preview/ECMLPKDD24.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ECMLPKDD24.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="da2024cityflower" class="col-sm-8"> <div class="title">CityFlowER: An Efficient and Realistic Traffic Simulator with Embedded Machine Learning Models</div> <div class="author"> <em>Longchao Da</em>, Chen Chu, Weinan Zhang , and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Hua Wei' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>arXiv preprint arXiv:2402.06127</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2402.06127" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://drive.google.com/drive/u/1/folders/1O6-HR8HgNoMEBzqAJWpRz5p6-3l7VtPy" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>Traffic simulation is an essential tool for transportation infrastructure planning, intelligent traffic control policy learning, and traffic flow analysis. Its effectiveness relies heavily on the realism of the simulators used. Traditional traffic simulators, such as SUMO and CityFlow, are often limited by their reliance on rule-based models with hyperparameters that oversimplify driving behaviors, resulting in unrealistic simulations. To enhance realism, some simulators have provided Application Programming Interfaces (APIs) to interact with Machine Learning (ML) models, which learn from observed data and offer more sophisticated driving behavior models. However, this approach faces challenges in scalability and time efficiency as vehicle numbers increase. Addressing these limitations, we introduce CityFlowER, an advancement over the existing CityFlow simulator, designed for efficient and realistic city-wide traffic simulation. CityFlowER innovatively pre-embeds ML models within the simulator, eliminating the need for external API interactions and enabling faster data computation. This approach allows for a blend of rule-based and ML behavior models for individual vehicles, offering unparalleled flexibility and efficiency, particularly in large-scale simulations. We provide detailed comparisons with existing simulators, implementation insights, and comprehensive experiments to demonstrate CityFlowER’s superiority in terms of realism, efficiency, and adaptability.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">da2024cityflower</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{CityFlowER: An Efficient and Realistic Traffic Simulator with Embedded Machine Learning Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Da, Longchao and Chu, Chen and Zhang, Weinan and Wei, Hua}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2402.06127}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CIKM24</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/LongchaoHere/assets/img/publication_preview/cikm2024-480.webp 480w,/LongchaoHere/assets/img/publication_preview/cikm2024-800.webp 800w,/LongchaoHere/assets/img/publication_preview/cikm2024-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/LongchaoHere/assets/img/publication_preview/cikm2024.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cikm2024.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="da2024shaded" class="col-sm-8"> <div class="title">Shaded Route Planning Using Active Segmentation and Identification of Satellite Images</div> <div class="author"> <em>Longchao Da</em>, Rohan Chhibba, Rushabh Jaiswal , and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Ariane Middel, Hua Wei' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>arXiv preprint arXiv:2407.13689</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://dl.acm.org/doi/abs/10.1145/3627673.3679234?casa_token=mK4O22u-8lcAAAAA:IASJiTtgTzsDp1snKw-AtwP-s4-QDSt-FyZ7Q65rQb47ofeN00odMMosTh3vv472MKL19sxSWbw" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://youtu.be/vdESBPGD4Lk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>Heatwaves pose significant health risks, particularly due to prolonged exposure to high summer temperatures. Vulnerable groups, especially pedestrians and cyclists on sun-exposed sidewalks, motivate the development of a route planning method that incorporates somatosensory temperature effects through shade ratio consideration. This paper is the first to introduce a pipeline that utilizes segmentation foundation models to extract shaded areas from high-resolution satellite images. These areas are then integrated into a multi-layered road map, enabling users to customize routes based on a balance between distance and shade exposure, thereby enhancing comfort and health during outdoor activities. Specifically, we construct a graph-based representation of the road map, where links indicate connectivity and are updated with shade ratio data for dynamic route planning. This system is already implemented online, with a video demonstration, and will be specifically adapted to assist travelers during the 2024 Olympic Games in Paris.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">da2024shaded</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Shaded Route Planning Using Active Segmentation and Identification of Satellite Images}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Da, Longchao and Chhibba, Rohan and Jaiswal, Rushabh and Middel, Ariane and Wei, Hua}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2407.13689}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">tutorial</span> <span class="p">=</span> <span class="s">{http://18.191.152.144:9999/tutorial.html}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%64%61%6C%6F%6E%67%63%68%61%6F@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=jic73NsAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="/LongchaoHere/feed.xml" title="RSS Feed"><i class="fa-solid fa-square-rss"></i></a> </div> <div class="contact-note">If you wanna reach out, please drop an email to: dalongchao[at]gmail[dot]com </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Longchao Da. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/LongchaoHere/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/LongchaoHere/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/LongchaoHere/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/LongchaoHere/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/LongchaoHere/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/LongchaoHere/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/LongchaoHere/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/LongchaoHere/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/LongchaoHere/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/LongchaoHere/"}},{id:"nav-publications",title:"Publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/LongchaoHere/publications/"}},{id:"nav-cv",title:"CV",description:"You can also download the pdf file from the top button.",section:"Navigation",handler:()=>{window.location.href="/LongchaoHere/cv/"}},{id:"nav-service",title:"Service",description:"Research community service.",section:"Navigation",handler:()=>{window.location.href="/LongchaoHere/service/"}},{id:"nav-teaching",title:"Teaching",description:"",section:"Navigation",handler:()=>{window.location.href="/LongchaoHere/teaching/"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:'Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/LongchaoHere/blog/2024/tabs/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/LongchaoHere/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/LongchaoHere/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/LongchaoHere/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/LongchaoHere/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/LongchaoHere/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/LongchaoHere/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/LongchaoHere/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/LongchaoHere/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/LongchaoHere/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/LongchaoHere/blog/2023/tikzjax/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/LongchaoHere/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/LongchaoHere/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/LongchaoHere/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/LongchaoHere/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/LongchaoHere/blog/2023/videos/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/LongchaoHere/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/LongchaoHere/blog/2023/table-of-contents/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/LongchaoHere/blog/2022/giscus-comments/"}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:'Displaying External Posts on Your al-folio Blog <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/LongchaoHere/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/LongchaoHere/blog/2021/diagrams/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/LongchaoHere/blog/2021/distill/"}},{id:"post-a-post-with-github-metadata",title:"a post with github metadata",description:"a quick run down on accessing github metadata.",section:"Posts",handler:()=>{window.location.href="/LongchaoHere/blog/2020/github-metadata/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/LongchaoHere/blog/2020/twitter/"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/LongchaoHere/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/LongchaoHere/blog/2015/math/"}},{id:"post-a-post-with-code",title:"a post with code",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/LongchaoHere/blog/2015/code/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/LongchaoHere/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march & april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/LongchaoHere/blog/2015/formatting-and-links/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/LongchaoHere/news/announcement_2/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%64%61%6C%6F%6E%67%63%68%61%6F@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=jic73NsAAAAJ","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/LongchaoHere/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/LongchaoHere/assets/js/shortcut-key.js"></script> </body> </html>