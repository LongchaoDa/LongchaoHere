---
---

@string{aps = {American Physical Society,}}


@inproceedings{da2024prompt,
  bibtex_show={true},
  title={Prompt to Transfer: Sim-to-Real Transfer for Traffic Signal Control with Prompt Learning},
  author={Da, Longchao and Gao, Minquan and Mei, Hao and Wei, Hua},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2024},
  abbr={AAAI24},
  html={https://ojs.aaai.org/index.php/AAAI/article/view/27758},
  doi={10.1609/aaai.v38i1.27758},
  abstract={Numerous solutions are proposed for the Traffic Signal Con- trol (TSC) tasks aiming to provide efficient transportation and alleviate traffic congestion. Recently, promising results have been attained by Reinforcement Learning (RL) methods through trial and error in simulators, bringing confidence in solving cities' congestion problems. However, performance gaps still exist when simulator-trained policies are deployed to the real world. This issue is mainly introduced by the sys- tem dynamic difference between the training simulators and the real-world environments. In this work, we leverage the knowledge of Large Language Models (LLMs) to understand and profile the system dynamics by a prompt-based grounded action transformation to bridge the performance gap. Specifi- cally, this paper exploits the pre-trained LLM’s inference abil- ity to understand how traffic dynamics change with weather conditions, traffic states, and road types. Being aware of the changes, the policies’ action is taken and grounded based on realistic dynamics, thus helping the agent learn a more realistic policy. We conduct experiments on four different scenarios to show the effectiveness of the proposed PromptGAT's ability to mitigate the performance gap of reinforcement learning from simulation to reality (sim-to-real).},
  preview={wave-mechanics.gif},
  selected={true},
}


@inproceedings{da2024probabilistic,
  bibtex_show={true},
  title={Probabilistic Offline Policy Ranking with Approximate Bayesian Computation},
  author={Da, Longchao and Jenkins, Porter and Schwantes, Trevor and Dotson, Jeffrey and Wei, Hua},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={18},
  pages={20370--20378},
  year={2024},
  abbr={AAAI24},
  html={https://ojs.aaai.org/index.php/AAAI/article/view/30019},
  doi={10.1609/aaai.v38i18.30019},
  abstract = {In practice, it is essential to compare and rank candidate policies offline before real-world deployment for safety and reliability. Prior work seeks to solve this offline policy ranking (OPR) problem through value-based methods, such as Off-policy evaluation (OPE). However, they fail to analyze special case performance (e.g., worst or best cases), due to the lack of holistic characterization of policies’ performance. It is even more difficult to estimate precise policy values when the reward is not fully accessible under sparse settings. In this paper, we present Probabilistic Offline Policy Ranking (POPR), a framework to address OPR problems by leveraging expert data to characterize the probability of a candidate policy behaving like experts, and approximating its entire performance posterior distribution to help with ranking. POPR does not rely on value estimation, and the derived performance posterior can be used to distinguish candidates in worst-, best-, and average-cases. To estimate the posterior, we propose POPR-EABC, an Energy-based Approximate Bayesian Computation (ABC) method conducting likelihood-free inference. POPR-EABC reduces the heuristic nature of ABC by a smooth energy function, and improves the sampling efficiency by a pseudo-likelihood. We empirically demonstrate that POPR-EABC is adequate for evaluating policies in both discrete and continuous action spaces across various experiment environments, and facilitates probabilistic comparisons of candidate policies before deployment.},
  preview={wave-mechanics.gif},
  selected={true},
}